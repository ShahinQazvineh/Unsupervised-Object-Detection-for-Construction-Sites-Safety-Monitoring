{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised PPE Detection (Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/drive/My Drive/ppe_detection') # Adjust this path to your project location\n",
    "from ppe_detection.config import load_config\n",
    "from ppe_detection.data_utils import prepare_dataset\n",
    "from ppe_detection.unsupervised_trainer import UnsupervisedTrainer\n",
    "from ppe_detection.discovery_processor import DiscoveryProcessor\n",
    "from ppe_detection.violation_processor import ViolationProcessor\n",
    "\n",
    "# Load the configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training overrides\n",
    "training_overrides = {\n",
    "    'frozen_layers': 12, # Example of overriding a parameter\n",
    "    'data_fraction': 0.5 # Example of using a subset of data\n",
    "}\n",
    "config['model']['frozen_layers'] = training_overrides.get('frozen_layers', config['model']['frozen_layers'])\n",
    "config['training']['data_fraction'] = training_overrides.get('data_fraction', config['training']['data_fraction'])\n",
    "\n",
    "# Prepare the dataset\n",
    "data, labels = prepare_dataset(config['data_dir'], config['training']['data_fraction'])\n",
    "# In a real scenario, you would create a proper data loader here\n",
    "import torch\n",
    "dummy_dataset = torch.utils.data.TensorDataset(torch.randn(len(data), 3, 518, 518), torch.tensor(labels))\n",
    "data_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=config['training']['batch_size'])\n",
    "\n",
    "# Initialize and run the trainer\n",
    "run_training = True\n",
    "if run_training:\n",
    "    trainer = UnsupervisedTrainer(config)\n",
    "    trainer.train(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discovery and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model (optional)\n",
    "model_path = 'output/checkpoints/latest_checkpoint.pt'\n",
    "discovery_processor = DiscoveryProcessor(config, model_path=model_path)\n",
    "\n",
    "# In a real scenario, you would load an image and generate masks\n",
    "import numpy as np\n",
    "dummy_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "masks = discovery_processor.generate_object_masks(dummy_image)\n",
    "\n",
    "# Manual class mapping (example)\n",
    "# This step would involve visualizing the discovered clusters and assigning labels.\n",
    "class_map = {1: 'person', 2: 'helmet', 3: 'vest'}\n",
    "config['discovery']['class_map'] = class_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Violation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_processor = ViolationProcessor(config)\n",
    "\n",
    "# In a real scenario, you would process a video frame by frame\n",
    "# For now, we'll use a dummy example\n",
    "discovered_objects = [\n",
    "    {'class_name': 'person', 'box': [100, 100, 200, 400], 'cluster_id': 1},\n",
    "    {'class_name': 'vest', 'box': [120, 150, 180, 250], 'cluster_id': 3},\n",
    "    {'class_name': 'person', 'box': [300, 100, 400, 400], 'cluster_id': 1},\n",
    "]\n",
    "\n",
    "labeled_objects = discovery_processor.apply_class_map(discovered_objects)\n",
    "violations = violation_processor.process_violations(labeled_objects, dummy_image)\n",
    "print(violations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
