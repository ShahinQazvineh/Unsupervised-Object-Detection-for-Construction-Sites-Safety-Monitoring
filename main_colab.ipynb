{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised PPE Detection (Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Robust Path Setup ---\n",
    "# Assumes this notebook is in the project root.\n",
    "# Get the absolute path of the current working directory.\n",
    "project_root = Path(os.getcwd())\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root set to: {project_root}\")\n",
    "print(f\"System path updated: {sys.path[0]}\")\n",
    "\n",
    "\n",
    "from config import CONFIG\n",
    "from data_utils import prepare_dataset\n",
    "from unsupervised_trainer import UnsupervisedTrainer\n",
    "from discovery_processor import DiscoveryProcessor\n",
    "from violation_processor import ViolationProcessor\n",
    "\n",
    "# --- Override Config Paths at Runtime ---\n",
    "# The config file might have a hardcoded path. We'll override it here\n",
    "# to ensure it uses the dynamically found project root.\n",
    "CONFIG['project_root_path'] = str(project_root)\n",
    "root = Path(CONFIG['project_root_path'])\n",
    "out_base_abs = root / CONFIG['output_dir']\n",
    "\n",
    "CONFIG['data_dir_abs'] = root / CONFIG['data_dir']\n",
    "CONFIG['output_dir_abs'] = out_base_abs\n",
    "CONFIG['checkpoint_dir_abs'] = out_base_abs / CONFIG['checkpoint_dir']\n",
    "\n",
    "print(f\"CONFIG 'data_dir_abs' updated to: {CONFIG['data_dir_abs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training overrides\n",
    "training_overrides = {\n",
    "    'frozen_layers': 12, # Example of overriding a parameter\n",
    "    'data_fraction': 0.5 # Example of using a subset of data\n",
    "}\n",
    "CONFIG['model']['frozen_layers'] = training_overrides.get('frozen_layers', CONFIG['model']['frozen_layers'])\n",
    "CONFIG['training']['data_fraction'] = training_overrides.get('data_fraction', CONFIG['training']['data_fraction'])\n",
    "\n",
    "# Prepare the dataset\n",
    "image_paths, labels = prepare_dataset(CONFIG['training']['data_fraction'])\n",
    "\n",
    "# Create a PyTorch dataset and dataloader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, labels\n",
    "\n",
    "class PpeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# You would define your transforms here\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((518, 518)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = PpeDataset(image_paths, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=CONFIG['training']['batch_size'], shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Initialize and run the trainer\n",
    "run_training = True\n",
    "if run_training:\n",
    "    trainer = UnsupervisedTrainer(CONFIG)\n",
    "    trainer.train(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discovery and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model (optional)\n",
    "model_path = 'output/checkpoints/latest_checkpoint.pt'\n",
    "discovery_processor = DiscoveryProcessor(CONFIG, model_path=model_path)\n",
    "\n",
    "# Load a sample image from the validation set for discovery\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "valid_image_paths = [p for p in image_paths if 'valid' in p]\n",
    "if valid_image_paths:\n",
    "    sample_image_path = random.choice(valid_image_paths)\n",
    "    print(f\"Using sample image for discovery: {sample_image_path}\")\n",
    "    sample_image = cv2.imread(sample_image_path)\n",
    "    sample_image_rgb = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "    masks = discovery_processor.generate_object_masks(sample_image)\n",
    "\n",
    "    # --- Visualize the results ---\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(sample_image_rgb)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(sample_image_rgb)\n",
    "    plt.imshow(masks, cmap='jet', alpha=0.5) # Overlay masks\n",
    "    plt.title('Image with Discovered Masks')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No validation images found. Skipping mask generation.\")\n",
    "\n",
    "# Manual class mapping (example) - This would be done after visualizing masks\n",
    "class_map = {1: 'person', 2: 'helmet', 3: 'vest'} # Example\n",
    "CONFIG['discovery']['class_map'] = class_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Violation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_processor = ViolationProcessor(CONFIG)\n",
    "\n",
    "# --- Video Processing ---\n",
    "# To process a video, provide the path to your video file below.\n",
    "# The code will loop through each frame, detect objects, and check for violations.\n",
    "\n",
    "video_path = 'path/to/your/video.mp4' # <--- CHANGE THIS PATH\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"Video file not found at {video_path}. Skipping inference.\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. Discover objects in the frame (this is a placeholder for the actual discovery output)\n",
    "        # In a real pipeline, you would run your object discovery model on the 'frame'\n",
    "        # and get a list of discovered objects.\n",
    "        discovered_objects = [] # Placeholder\n",
    "        \n",
    "        # 2. Apply class map\n",
    "        labeled_objects = discovery_processor.apply_class_map(discovered_objects)\n",
    "        \n",
    "        # 3. Process violations\n",
    "        violations = violation_processor.process_violations(labeled_objects, frame)\n",
    "        \n",
    "        # 4. (Optional) Visualize the output\n",
    "        # You can draw bounding boxes and violation alerts on the frame here.\n",
    "        \n",
    "    cap.release()\n",
    "    print(\"Video processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation ---\n",
    "# This cell evaluates the model's performance on the validation set.\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    all_ious = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            \n",
    "            # Note: This is a placeholder for your model's prediction logic.\n",
    "            # The output format will depend on your actual model.\n",
    "            # For this example, we assume the model returns boxes in the same format as the target.\n",
    "            predictions = model(images) # This line is illustrative\n",
    "\n",
    "            for i, target in enumerate(targets):\n",
    "                pred_boxes = predictions[i]['boxes']\n",
    "                target_boxes = target['boxes']\n",
    "                for t_box in target_boxes:\n",
    "                    best_iou = 0\n",
    "                    for p_box in pred_boxes:\n",
    "                        iou = calculate_iou(t_box, p_box)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                    all_ious.append(best_iou)\n",
    "\n",
    "    mean_iou = np.mean(all_ious) if all_ious else 0\n",
    "    print(f\"Mean IoU on Validation Set: {mean_iou:.4f}\")\n",
    "    return mean_iou\n",
    "\n",
    "# Create a new DataLoader for the validation set\n",
    "valid_dataset = PpeDataset([p for p in image_paths if 'valid' in p], [l for i, l in enumerate(labels) if 'valid' in image_paths[i]], transform=transform)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=CONFIG['training']['batch_size'], collate_fn=custom_collate_fn)\n",
    "\n",
    "# Run evaluation\n",
    "# Note: The 'evaluate' function needs a model that produces predictions.\n",
    "# The current UnsupervisedTrainer doesn't have a prediction method, so this is a template.\n",
    "print(\"Validation logic is set up. You would call the 'evaluate' function with your trained model.\")\n",
    "# evaluate(trainer.model, validation_loader, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
