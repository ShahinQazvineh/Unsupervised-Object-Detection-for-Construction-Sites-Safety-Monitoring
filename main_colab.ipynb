{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised PPE Detection (Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Robust Path Setup ---\n",
    "# Assumes this notebook is in the project root.\n",
    "# Get the absolute path of the current working directory.\n",
    "project_root = Path(os.getcwd())\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root set to: {project_root}\")\n",
    "print(f\"System path updated: {sys.path[0]}\")\n",
    "\n",
    "\n",
    "from config import CONFIG\n",
    "from data_utils import prepare_dataset\n",
    "from unsupervised_trainer import UnsupervisedTrainer\n",
    "from discovery_processor import DiscoveryProcessor\n",
    "from violation_processor import ViolationProcessor\n",
    "\n",
    "# --- Override Config Paths at Runtime ---\n",
    "# The config file might have a hardcoded path. We'll override it here\n",
    "# to ensure it uses the dynamically found project root.\n",
    "CONFIG['project_root_path'] = str(project_root)\n",
    "root = Path(CONFIG['project_root_path'])\n",
    "out_base_abs = root / CONFIG['output_dir']\n",
    "\n",
    "CONFIG['data_dir_abs'] = root / CONFIG['data_dir']\n",
    "CONFIG['output_dir_abs'] = out_base_abs\n",
    "CONFIG['checkpoint_dir_abs'] = out_base_abs / CONFIG['checkpoint_dir']\n",
    "\n",
    "print(f\"CONFIG 'data_dir_abs' updated to: {CONFIG['data_dir_abs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training overrides\n",
    "training_overrides = {\n",
    "    'frozen_layers': 12, # Example of overriding a parameter\n",
    "    'data_fraction': 0.5 # Example of using a subset of data\n",
    "}\n",
    "CONFIG['model']['frozen_layers'] = training_overrides.get('frozen_layers', CONFIG['model']['frozen_layers'])\n",
    "CONFIG['training']['data_fraction'] = training_overrides.get('data_fraction', CONFIG['training']['data_fraction'])\n",
    "\n",
    "# Prepare the dataset\n",
    "image_paths, labels = prepare_dataset(CONFIG['training']['data_fraction'])\n",
    "\n",
    "# Create a PyTorch dataset and dataloader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class PpeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# You would define your transforms here\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((518, 518)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = PpeDataset(image_paths, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=CONFIG['training']['batch_size'], shuffle=True)\n",
    "\n",
    "# Initialize and run the trainer\n",
    "run_training = True\n",
    "if run_training:\n",
    "    trainer = UnsupervisedTrainer(CONFIG)\n",
    "    trainer.train(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discovery and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model (optional)\n",
    "model_path = 'output/checkpoints/latest_checkpoint.pt'\n",
    "discovery_processor = DiscoveryProcessor(CONFIG, model_path=model_path)\n",
    "\n",
    "# Load a sample image from the validation set for discovery\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "valid_image_paths = [p for p in image_paths if 'valid' in p]\n",
    "if valid_image_paths:\n",
    "    sample_image_path = random.choice(valid_image_paths)\n",
    "    print(f\"Using sample image for discovery: {sample_image_path}\")\n",
    "    sample_image = cv2.imread(sample_image_path)\n",
    "    masks = discovery_processor.generate_object_masks(sample_image)\n",
    "else:\n",
    "    print(\"No validation images found. Skipping mask generation.\")\n",
    "\n",
    "# Manual class mapping (example) - This would be done after visualizing masks\n",
    "class_map = {1: 'person', 2: 'helmet', 3: 'vest'} # Example\n",
    "CONFIG['discovery']['class_map'] = class_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Violation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_processor = ViolationProcessor(CONFIG)\n",
    "\n",
    "# --- Video Processing ---\n",
    "# To process a video, provide the path to your video file below.\n",
    "# The code will loop through each frame, detect objects, and check for violations.\n",
    "\n",
    "video_path = 'path/to/your/video.mp4' # <--- CHANGE THIS PATH\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"Video file not found at {video_path}. Skipping inference.\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. Discover objects in the frame (this is a placeholder for the actual discovery output)\n",
    "        # In a real pipeline, you would run your object discovery model on the 'frame'\n",
    "        # and get a list of discovered objects.\n",
    "        discovered_objects = [] # Placeholder\n",
    "        \n",
    "        # 2. Apply class map\n",
    "        labeled_objects = discovery_processor.apply_class_map(discovered_objects)\n",
    "        \n",
    "        # 3. Process violations\n",
    "        violations = violation_processor.process_violations(labeled_objects, frame)\n",
    "        \n",
    "        # 4. (Optional) Visualize the output\n",
    "        # You can draw bounding boxes and violation alerts on the frame here.\n",
    "        \n",
    "    cap.release()\n",
    "    print(\"Video processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
